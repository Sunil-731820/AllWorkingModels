<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Efesto refactoring &amp;#8211; technical details</title><link rel="alternate" href="https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html" /><author><name>Gabriele Cardosi</name></author><id>https://blog.kie.org/2022/09/efesto-refactoring-technical-details.html</id><updated>2022-09-21T11:09:56Z</updated><content type="html">This post is meant as a description of the APIs and other technical details of the Efesto framework. It continues the introduction made in the BASE CONCEPTS. There are some concepts around which the APIs are implemented: * Generated resource * Unique identifier * Context of execution The framework provides and manage default implementations of the classes representing those concepts. Those classes could be extended by different engines for their specific needs (e.g. the Kie-drl compilation plugin define a context that contains a KnowledgeBuilderConfiguration) but this specific addition should never leak out of the engine itself, and the functionality of the framework itself should never rely on such "custom" details. GENERATED RESOURCE A represent the result of a compilation. By itself is just a marker interface because there are different kind of generated resources: * executable resources () * redirect resources () * “container” resources (like ). Executable resources represents the "entry point" for execution at runtime, and it contains information required to "instantiate" the executable unit. For some code-generation models (e.g. rules, predictions) this means store the class to instantiate at runtime, that will be used to start the evaluation. For models that does not rely on code-generation for execution (e.g. decisions), this resource contains the name of the class to be instantiated and/or the methods/parameters to be invoked. Redirect resources contains information needed to forward the execution request to a different engine, and it contains the informatio about the ewngine to be invoked. Container resources are meant to store other informations needed at runtime (e.g. the classes generated during compilation). UNIQUE IDENTIFIER The unique identifier () contains the information required to uniquely identify an executable or redirect generated resource. ModelLocalUriId contains information about: * the model/engine to invoke * the full path to the given resource The unique identifier is represented a "path" whose root is the model/engine to invoke, and the path describe all the elements required to get to the specific resource. Stateless engines (e.g. DMN, PMML) describe that as "/namespace/model_name" or "/filename/model_name". Statefull engines would require further path compoenents to identify the specific "state" to be invoked (e.g. "/drl/rule_base/session_name/session_identifier"). ModelLocalUriId is a property of both GeneratedExecutableResource and GeneratedRedirectResource, since both of them have to be retrieved during runtime execution. ModelLocalUriId implements and is a feature that was initially implemented in the Kogito Incubation API, for which an explanation is available . For each module, client code should be able to invoke a method like that to retrieve the unique identifier: ModelLocalUriId modelLocalUriId = appRoot("") .get(PmmlIdFactory.class) .get(fileNameNoSuffix, modelName); This is a fluent API, and each get invocation corresponds to an element in the generated path. The appRoot parameter is only used to differentiate multiple applications (e.g. in distributed context). The first get is needed to start the path building. Each module should implement its own factory extending , that, in turn, will be used to generate the full path. Each of the following get should return an object that extends ModelLocalUriId, since each it represent the path until that specific segment. Each module may provide its own strategy to define such paths, so each module may implement its own subclasses, depending on the needs. Since the The ModelLocalUriId constructor requires a instance, any of its subclasses should implement a way to call that constructor with such instance. In the following example: public class PmmlIdFactory implements ComponentRoot { public LocalComponentIdPmml get(String fileName, String name) { return new LocalComponentIdPmml(fileName, name); } } the PmmlIdFactory expose a get method ( the fluent API) that requires fileName and name parameters. This, in turns, are used to invoke the LocalComponentIdPmml constructor. public class LocalComponentIdPmml extends ModelLocalUriId { public static final String PREFIX = "pmml"; public LocalComponentIdPmml(String fileName, String name) { super(LocalUri.Root.append(PREFIX).append(fileName).append(name)); } } This snippet: LocalUri.Root.append(PREFIX).append(fileName).append(name) will lead to the creation of the following path: /{PREFIX}/{fileName}/{name} CONTEXT OF EXECUTION. The contains basic information about the current execution. It contains informations about the generated classes and the unique identifiers generated during compilation. is the specialization used at runtime to retrieve the generated classes. is the default implementation. Engines may extends the above as per their needs. For example, (the EfestoCompilationContext used inside the rule engine) defines KnowledgeBuilderConfiguration for its needs. COMPILATION CONTEXT is the specialization used at compile time, and it is used to store the classes generated during compilation. is the default implementation. provide a static method to retrieve the default implementation () with all the classes eventually compiled from a previous compilation. That static method, behind the scenes, invokes the constructor that scan the classloader to look for efesto-related classes. RUNTIME CONTEXT is the specialization used at runtime to retrieve the generated classes. is the default implementation. provide a static method to retrieve the default implementation () with all the efesto-related compiled classes. That static method, behind the scenes, invokes the constructor that scan the classloader to look for efesto-related classes. PUBLIC APIS The framework consists basically of two set of APIs, the "compilation" and the "runtime" ones. Those APIs are defined inside and . Those are the APIs that "client code" is expected to invoke. Said differently, "client code" is expected to interact with engines only through those APIs. COMPILATION API void processResource(EfestoCompilationContext context, EfestoResource... toProcess); This is the method that "External applications" (e.g. kie-maven-plugin) should invoke to create executables units out of given models. is the DTO wrapping a single model to be processed. Its only method T getContent(); is invoked by the compilation manager to get the object to be processed. The more common usage is to provide an actual File to the compilation manager, in which case there already is an implementation, . is a specific abstract implementations that wraps a Set of models. As for the previous, there already exist an implementation to manage FIles, . RUNTIME API Collection&lt;EfestoOutput&gt; evaluateInput(EfestoRuntimeContext context, EfestoInput... toEvaluate); This is the method that "External applications" (e.g. kogito execution) should invoke to retrieve a result out of executable units generated at compile-time. is the DTO wrapping a the data to be evaluated and the unique identifier of the executable units. It has two methods: ModelLocalUriId getModelLocalUriId(); T getInputData(); the former returns the unique identifier of the executable units; the latter returns the data to use for evaluation. Currently there are no "default" implementations of it, since the input structure is generally model-specific; so, every plugin should provide its own implementation. INTERNAL APIS Behind the scenes, when CompilationManager and RuntimeManager receives a request, they scan the classloader for engine plugins. Such plugins should implement, respectively, the and the . COMPILERSERVICE API declares three methods: boolean canManageResource(EfestoResource toProcess); List&lt;E&gt; processResource(EfestoResource toProcess, U context); String getModelType(); The first one is invoked by the CompilationManager to verify if the specific implementation is able to manage the given resource. The evaluation could be based on the actual type of the resource, on some details of the content, or on a mix of them. It is responsibility of the implementation to find the appropriate logic. The only requirement to keep in mind is that, during execution, there should be at most one implementation that return true for a given EfestoResource, otherwise an exception is thrown. The following snippet is an example where a given EfestoResource is considered valid if it is an DecisionTableFileSetResource: @Override public boolean canManageResource(EfestoResource toProcess) { return toProcess instanceof DecisionTableFileSetResource; } The above implementation works because DecisionTableFileSetResource is a class specifically defined by the plugin itself, so there are no possible "overlaps" with other implementations. On the other side, the following snippet is an example where a given EfestoResource is considered valid if it is an EfestoFileResource and if the contained model is a PMML: @Override public boolean canManageResource(EfestoResource toProcess) { return toProcess instanceof EfestoFileResource &amp;amp;&amp; ((EfestoFileResource) toProcess).getModelType().equalsIgnoreCase(PMML_STRING); } In this case, the actual class of EfestoResource is not enough, since EfestoFileResource is one of the default implementations provided by the framework. So, a further check is needed, that is about the model that is wrapped in the resource. A single plugin may manage multiple representations of the same model. For example, a plugin may manage both an EfestoFileResource and an EfestoInputStreamResource. There are different possible strategies to do that. For example, the plugin may provide one single "compilation-module" with two classes implementing the KieCompilerService; or it may define two "compilation-modules", each of which with one implementation, or one single class may manage both kind of inputs. Again, this is responsibility of the plugin itself. This also push toward code reusage. For a given model, there could be a common path that provide the final compilation output, and different entry point depending on the model representation. It is so possible that multiple compilation models creates a compilation output that, in turns, it is also an EfestoResource. Then, there could be another implementation that accept as input the above intermediate resuorce, and transform it to the final compilation outpout. This chaining is managed by the efesto framework out of the box. An example of that is featured by drools-related pmml models. During compilation, the PMML compiler generates an that is both an EfestoResource and an EfestoCompilationOutput. When the CompilationManager retrieves that compilation output, being it an EfestoResource, scans the plugins to find someone that is able to compile it. The fullfill this requirement, and proceed with drl-specific compilation. One thing to notice here is that different modules should limit as much as possible direct dependency between them. The second method is invoked by the compilation manager if the previous one returned true. That method receives also an EfestoCompilationContext as parameter. Code-generating implementations should rely on that context for compilation and classloading. The third method is used by the framework to discover, at execution time, which models can actually be managed. Thanks to that method, there is a complete de-coupling between the framework and the implementation themselves, since the framework can discover dynamically the available models, and every plugin may freely define its own model. Last critical bit is that every compilation module should contain an org.kie.efesto.compilationmanager.api.service.KieCompilerService file inside src/main/resources/META-INF directory, and that file should contain all the KieCompilationService implementations provided by that module. RUNTIMESERVICE API declares three methods: boolean canManageInput(EfestoInput toEvaluate, K context); Optional&lt;E&gt; evaluateInput(T toEvaluate, K context); String getModelType(); The first one is invoked by the RuntimeManager to verify if the specific implementation is able to manage the given input. The evaluation could be based on the actual type of the resource, on some details of the content, or on a mix of them. It is responsibility of the implementation to find the appropriate logic. The only requirement to keep in mind is that, during execution, there should be at most one implementation that return true for a given EfestoInput, otherwise an exception is thrown. The following snippet is an example where a given EfestoInput is considered valid if it is an EfestoInputPMML and the given identifier has already been compiled: public static boolean canManage(EfestoInput toEvaluate, EfestoRuntimeContext runtimeContext) { return (toEvaluate instanceof EfestoInputPMML) &amp;amp;&amp; isPresentExecutableOrRedirect(toEvaluate.getModelLocalUriId(), runtimeContext); } The above implementation works because EfestoInputPMML is a class specifically defined by the plugin itself, so there are no possible "overlaps" with other implementations. The difference with the compilation side is that the KieRuntimeService implementation should also check that the model related to the given unique identifier has already been compiled. A single plugin may manage different types of input for the same model. For example, the rule plugin may manage both an EfestoInputDrlKieSessionLocal and an AbstractEfestoInput that contains an EfestoMapInputDTO. There are different possible strategies to do that. For example, the plugin may provide one single "runtime-module" with two classes implementing the KieRuntimeService; or it may define two "runtime-modules", each of which with one implementation, or one single class may manage both kind of inputs. Again, this is responsibility of the plugin itself. This also push toward code reusage. For a given model, there could be a common code-path that provides the final runtime result, and different entry point depending on the input format. It is so possible that a runtime implementation would need a result from another implementation. In that case, the calling runtime will create a specifically-crafted EfestoInput and will ask the RuntimeManage the result for it. This chaining is managed by the efesto framework out of the box. An example of that is featured by drools-related pmml models. During execution, the PMML runtime generates an EfestoInput&lt;EfestoMapInputDTO&gt; and send it to the RuntimeManager. The RuntimeManager scans the plugins to find someone that is able to execute it. The fullfill this requirement, and proceed with drl-specific execution. One thing to note here is thet modules should limit as much as possible direct dependency between them! The second method is invoked by the runtime manager if the previous one returned true. That method receives also an EfestoRuntimeContext as parameter. Code-generating implementations should rely on that context to retrieve/load classes generated during compilation. The third method is used by the framework to discover, at execution time, which models can actually be managed. Thanks to that method, there is a complete de-coupling between the framework and the implementation themselves, since the framework can discover dynamically the available models, and every plugin may freely define its own model. Last critical bit is that every compilation module should contain an org.kie.efesto.runtimemanager.api.service.KieRuntimeService file inside src/main/resources/META-INF directory, and that file should contain all the KieRuntimeService implementations provided by that module. CONCLUSION This post was meant to provide more technical details on what have been introduced in the . Following ones will provide concrete step-by-step tutorial and real uses-cases so… stay tuned!!! The post appeared first on .</content><dc:creator>Gabriele Cardosi</dc:creator></entry><entry><title>Bind services created with AWS Controllers for Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/21/bind-services-created-aws-controllers-kubernetes" /><author><name>Baiju Muthukadan</name></author><id>ea05a1b5-94fc-4f41-9f45-43a331d6a4c7</id><updated>2022-09-21T07:00:00Z</updated><published>2022-09-21T07:00:00Z</published><summary type="html">&lt;p&gt;Application developers can define Amazon Web Services (AWS) resources directly from Kubernetes using &lt;a href="https://aws-controllers-k8s.github.io/community/docs/community/overview/"&gt;AWS Controllers for Kubernetes&lt;/a&gt; (ACK). You can use the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/intro.html"&gt;Service Binding Operator&lt;/a&gt; to easily connect applications to any AWS service provisioned through ACK.&lt;/p&gt; &lt;p&gt;This article explores the connection with an RDS database and demonstrates configuring ACK to create a service instance for the AWS Relational Database Service (RDS). You can also learn how to use Service Binding Operator annotations to bind a PostgreSQL service created using RDS and a REST API.&lt;/p&gt; &lt;h2&gt;Benefits of the Service Binding Operator and AWS Controllers for Kubernetes &lt;/h2&gt; &lt;p&gt;One benefit of the Service Binding Operator and ACK is that they streamline the formation of a connection. The Service Binding Operator implements the &lt;a href="https://servicebinding.io"&gt;Service Binding specification for Kubernetes&lt;/a&gt;. This is a Kubernetes-wide specification for automating the process of service secrets communicating to workloads.&lt;/p&gt; &lt;p&gt;Another benefit of using the Service Binding Operator is that the only focus of applications with many microservices (maybe hundreds of them) is setting the correct label to receive binding data from the services specified by Service Binding Operator resources using the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/binding-workloads-using-sbo/binding-options.html#binding-workloads-using-a-label-selector"&gt;label selector&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Service Binding Operator supports the following methods to obtain connection details from a service:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/servicebinding/spec#provisioned-service"&gt;Provisioned Service&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/servicebinding/spec#direct-secret-reference"&gt;Direct Secret Reference&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/exposing-binding-data/adding-annotation.html"&gt;Annotations&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently, ACK does not support the Provisioned Service method. And no single secret contains all the connection details. In such a scenario, you can use the annotation support provided by the Service Binding Operator and add this annotation to a Custom Resource (CR) or Custom Resource Definition (CRD).&lt;/p&gt; &lt;p&gt;The following articles offer more information about ACK, including where the ACK project came from, why the Operator pattern is used, and how to configure and use ACK:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;Create AWS resources with Kubernetes and Operators&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Step 1:  Prerequisites setup&lt;/h2&gt; &lt;p&gt;The prerequisites for this demonstration are pretty simple. You must have an AWS account and a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster with the Service Binding Operator installed.&lt;/p&gt; &lt;h3&gt;AWS account permissions&lt;/h3&gt; &lt;p&gt;Your AWS account must have the &lt;a href="https://aws-controllers-k8s.github.io/community/docs/user-docs/authorization/#aws-iam-permissions-for-ack-controller"&gt;IAM role permissions&lt;/a&gt; for the Amazon Relational Database Service (RDS) ACK controller. The policy required for RDS is:&lt;/p&gt; &lt;p&gt;&lt;code&gt;arn:aws:iam::aws:policy/AmazonRDSFullAccess&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;OpenShift cluster with the Service Binding Operator&lt;/h3&gt; &lt;p&gt;You need administrator access to an OpenShift cluster. To install the Sevice Binding Operator, create a subscription similar to this example:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: my-service-binding-operator namespace: openshift-operators spec: channel: stable name: rh-service-binding-operator source: redhat-operators sourceNamespace: openshift-marketplace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, place this configuration in a file named &lt;code&gt;subscription.yaml&lt;/code&gt;. Then use the following &lt;code&gt;oc&lt;/code&gt; command to create the resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f subscription.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can install the Service Binding Operator from &lt;a href="https://operatorhub.io"&gt;OperatorHub&lt;/a&gt; using the OpenShift administrator console.&lt;/p&gt; &lt;h2&gt;Step 2:  Install the RDS Operator in an OpenShift cluster&lt;/h2&gt; &lt;p&gt;These four steps use the ACK Operator to install the RDS database. The official documentation shows detailed information about configuring ACK in an OpenShift cluster.&lt;/p&gt; &lt;h3&gt;1. Create a namespace&lt;/h3&gt; &lt;p&gt;The following example uses a namespace called &lt;code&gt;ack-system&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project ack-system&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the output you should see:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;Now using project "ack-system" on server "https://example.org:6443". ...&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;2. Create a config map&lt;/h3&gt; &lt;p&gt;Create a config map with the following content in a &lt;code&gt;config.txt&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=us-west-2 AWS_ENDPOINT_URL= ACK_RESOURCE_TAGS=hellofromocp&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use this config map in your OpenShift cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create configmap --namespace ack-system \ --from-env-file=config.txt ack-rds-user-config &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;3. Create a secret&lt;/h3&gt; &lt;p&gt;Save the following authentication values in a file, such as &lt;code&gt;secrets.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;AWS_ACCESS_KEY_ID=&lt;access key id&gt; AWS_SECRET_ACCESS_KEY=&lt;secret access key&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use this &lt;code&gt;secrets.txt&lt;/code&gt; file to create a secret in your OpenShift cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic \ --namespace ack-system \ --from-env-file=secrets.txt ack-rds-user-secrets&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Be sure to secure access to this resource and the namespace because you will keep sensitive information in this secret—your AWS Access Key ID and AWS Secret Access Key.&lt;/p&gt; &lt;p&gt;Alternatively, you can set up secure access using &lt;a href="https://aws-controllers-k8s.github.io/community/docs/user-docs/irsa/#create-an-iam-role-for-your-ack-service-controller"&gt;IAM Roles for Service Accounts&lt;/a&gt; (IRSA).&lt;/p&gt; &lt;h3&gt;4. Install the relational database service&lt;/h3&gt; &lt;p&gt;Refer to the article &lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;How to get Operators to use AWS Controllers for Kubernetes&lt;/a&gt; for ACK RDS controller installation instructions. After successful installation, this page (Figure 1) appears in the administrator console.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog-ack.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/blog-ack.png?itok=nbjyUns8" width="1440" height="710" alt="This page appears in the OpenShift administrator console after installation." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: After the ACK RDS controller is installed, this page appears in the OpenShift administrator console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Step 3:  The consumption of annotations and label selectors&lt;/h2&gt; &lt;p&gt;To enable binding, the Service Binding Operator uses the following annotations that are part of the &lt;code&gt;DBInstance&lt;/code&gt; resource in a &lt;a href="https://helm.sh"&gt;Helm chart&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: rds.services.k8s.aws/v1alpha1 kind: DBInstance metadata: annotations: "service.binding/type": "path={.spec.engine}" "service.binding/provider": "aws" "service.binding/host": "path={.status.endpoint.address}" "service.binding/port": "path={.status.endpoint.port}" "service.binding/username": "path={.spec.masterUsername}" "service.binding/password": 'path={.spec.masterUserPassword.name},objectType=Secret,sourceKey=password' "service.binding/database": "path={.spec.engine}" ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;DBInstance&lt;/code&gt; definition represents an AWS RDS resource.&lt;/p&gt; &lt;p&gt;To define the workload, the Service Binding Operator uses the following label selector (part of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Helm chart):&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: binding.operators.coreos.com/v1alpha1 kind: ServiceBinding metadata: name: servicebinding-rds-endpoint-demo spec: bindAsFiles: true services: - group: rds.services.k8s.aws version: v1alpha1 kind: DBInstance name: {{ .Values.dbinstance.name }} application: labelSelector: matchLabels: psql.provider: aws (*) version: v1 group: apps resource: deployments&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;(*) This line specifies the label that the Service Binding Operator uses to identify the workload.&lt;/p&gt; &lt;p&gt;The Helm charts are available in the &lt;a href="https://github.com/redhat-developer/openshift-app-services-demos"&gt;app-services-samples repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We have not deployed the application yet. Typically, the ServiceBinding controller waits for a workload resource with a matching &lt;code&gt;psql.provider: aws&lt;/code&gt; label. As soon as a workload resource is available with the matching label, the Operator uses the ServiceBinding controller to project the binding values to the workload.&lt;/p&gt; &lt;p&gt;The binding values projects into the &lt;code&gt;/bindings&lt;/code&gt; directory inside the container of the workload resource. The following directory structure stores the values:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;/bindings └── servicebinding-rds-endpoint-demo ├── type ├── database ├── host ├── username └── password&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The REST API application uses a suitable and compliant &lt;a href="https://servicebinding.io/application-developer/#language-specific-libraries"&gt;library&lt;/a&gt; to consume the projected binding values.&lt;/p&gt; &lt;h2&gt;Step 4:  Create a database instance&lt;/h2&gt; &lt;p&gt;After you clone the &lt;a href="https://github.com/redhat-developer/openshift-app-services-demos"&gt;app-services-samples repository&lt;/a&gt; described in the previous section, change to the &lt;code&gt;openshift-app-services-demos/samples/sbo/ack-rds-blog&lt;/code&gt; directory to perform these two steps:&lt;/p&gt; &lt;p&gt;1. Run Helm on the &lt;code&gt;rds-postgre-chart-demo&lt;/code&gt; chart:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm install rds-postgre-chart-demo -n ack-system rds-postgre-chart-demo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the output you should see:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME: rds-postgre-chart-demo LAST DEPLOYED: Thu Aug 4 09:29:26 2022 NAMESPACE: ack-system STATUS: deployed REVISION: 1 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2. Run the following command to validate the database instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get dbinstance rds-test-demo -n ack-system -o=jsonpath='{.status.dbInstanceStatus}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;available&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the database is ready to use.&lt;/p&gt; &lt;h2&gt;Step 5:  Deploy the REST API application&lt;/h2&gt; &lt;p&gt;In this demo, we use the Software Security Module (SSM), a Go-based REST API application. For convenience, deploy the application using the Helm chart in the &lt;a href="https://github.com/redhat-developer/openshift-app-services-demos"&gt;app-services-samples repository&lt;/a&gt;. After you clone the repository, perform the following steps from the &lt;code&gt;openshift-app-services-demos/samples/sbo/ack-rds-blog&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;1. Run Helm on the &lt;code&gt;ssm-chart&lt;/code&gt; chart:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm install ssm-chart -n ack-system ssm-chart&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME: ssm-chart LAST DEPLOYED: Thu Aug 4 04:22:24 2022 NAMESPACE: ack-system STATUS: deployed REVISION: 1 TEST SUITE: None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;2. Verify that the deployment of the REST API application is successful by running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get deployment -n ack-system&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME READY UP-TO-DATE AVAILABLE AGE ack-rds-controller 1/1 1 1 28m&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The deployment is defined as follows in the Helm chart:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Values.k8Name }} annotations: app.kubernetes.io/part-of: ssm labels: psql.provider: aws (*) ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(*) This line specifies the required matching label that the ServiceBinding controller uses to identify the workload and project the bindings.&lt;/p&gt; &lt;p&gt;The ServiceBinding controller watches for a deployment matching the label. After the deployment is ready, the Operator uses the ServiceBinding controller to project the binding values to the workload.&lt;/p&gt; &lt;h2&gt;Step 6:  Access and validate the REST API application&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;ssm-chart&lt;/code&gt; Helm chart also creates an &lt;code&gt;ssm&lt;/code&gt; service resource for convenient access to the application. The &lt;code&gt;ssm&lt;/code&gt; service resource points to the REST API application. Before connecting to this application, make sure you have the &lt;code&gt;DBInstance&lt;/code&gt; resource created and ready with an RDS instance provisioned in the AWS.&lt;/p&gt; &lt;p&gt;Switch to another terminal to run the commands in the following steps.&lt;/p&gt; &lt;h3&gt;1. Access the REST API application by forwarding the port of the service&lt;/h3&gt; &lt;p&gt;An &lt;code&gt;oc&lt;/code&gt; command on OpenShift is useful for port forwarding:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc port-forward --address 0.0.0.0 svc/ssm 8080:8080 -n ack-system&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;2. Validate the application&lt;/h3&gt; &lt;p&gt;Validate that the application works as follows:&lt;/p&gt; &lt;h4&gt;Generate a based64-encoded string&lt;/h4&gt; &lt;p&gt;Start by creating a string from random input:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ openssl rand 32 | base64&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This output contains the string you will use as input in the next step.:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;rgeR0ENzlxG+Erss6tw0gBkBWdLOPrQhEFQpH8O5t/Y=&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;h4&gt;Call the wrap API&lt;/h4&gt; &lt;p&gt;Call the application's &lt;code&gt;wrap&lt;/code&gt; API to create a cipher from the string by using the based64-encoded string from the previous step as input when calling the &lt;code&gt;wrap&lt;/code&gt; API:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl http://localhost:8080/wrap -d '{"key": "rgeR0ENzlxG+Erss6tw0gBkBWdLOPrQhEFQpH8O5t/Y="}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This output contains the cipher string you will use as input in the next step:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{"cipher":"D/S6wDJPH ... "}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;h4&gt;Call the unwrap API&lt;/h4&gt; &lt;p&gt;Now call the application's &lt;code&gt;unwrap&lt;/code&gt; API to restore the original based64 -encoded string by submitting the JSON from the output in the previous section to the &lt;code&gt;unwrap&lt;/code&gt; API:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl http://localhost:8080/unwrap -d '{"cipher":"D/S6wDJPH ... "}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output returns the original based64-encoded string:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{"key":"rgeR0ENzlxG+Erss6tw0gBkBWdLOPrQhEFQpH8O5t/Y="} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;h2&gt;The Service Binding Operator simplifies installation and deployment&lt;/h2&gt; &lt;p&gt;With the annotation support of the Service Binding Operator, you can easily bind ACK services without making any changes to the code. You can use the same label to bind any number of workloads. The REST API application consumes the projected binding values by using one of the &lt;a href="https://servicebinding.io/application-developer/#language-specific-libraries"&gt;libraries&lt;/a&gt; compliant with the Service Binding specification for Kubernetes. You can use the REST API application to connect to the AWS RDS service without any specific change.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/21/bind-services-created-aws-controllers-kubernetes" title="Bind services created with AWS Controllers for Kubernetes"&gt;Bind services created with AWS Controllers for Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Baiju Muthukadan</dc:creator><dc:date>2022-09-21T07:00:00Z</dc:date></entry><entry><title type="html">New visualizer for the Serverless Workflow Editor</title><link rel="alternate" href="https://blog.kie.org/2022/09/new-visualizer-for-the-serverless-workflow-editor.html" /><author><name>Roger Palleja</name></author><id>https://blog.kie.org/2022/09/new-visualizer-for-the-serverless-workflow-editor.html</id><updated>2022-09-21T00:08:57Z</updated><content type="html">We’re happy to announce that a new diagram visualizer for the domain has been released, as part of the kogito tooling 0.23.0, and It becomes as default for the . Kogito – Serverless Workflow Editor – VSCode extension If you are not familiar with the kogito tooling and its extensions, please refer to the guide first. A part from the previous capabilities of the editor, this new diagram visualizer provides a bunch of additional features to help users during the authoring of their workflows, such as: * Automatic workflow reloading It dynamically reloads the workflow’s visualization, once any change is being done in the JSON declaration text panel. * Error Handling In case the workflow’s JSON declaration is not valid (thus the workflow cannot be automatically reloaded), the editor presents the latest valid visualization for the workflow, and also an icon appears on the top right corner: On mouse over the error icon, it  displays the cause of the error as well, by showing a user friendly message. Once the diagram is valid again, the error icon will disappear, and the visualization will be properly updated. * State navigation Once an state is being selected in the diagram visualizer (by clicking on it), the editor automatically navigates to the line, in the JSON declaration, where the state is being defined. * Mediators Users are able to play with mediators either by using the mouse, or by using the available buttons in the mediators bar: * Auto-fit to diagram size: It fits the diagram to the actual viewport size * Zoom: Scales the viewport accordingly (also available by using mouse mediators, please see the keybindings page) * Panning: Translates the viewport accordingly (only available by using mouse mediators, please see the keybindings page) * Export  workflow to SVG From the technical perspective, just mention it is based on , and it relies on Canvas as the main rendering technology. Please keep posted on further updates, new features and improvements coming soon! The post appeared first on .</content><dc:creator>Roger Palleja</dc:creator></entry><entry><title>Quarkus 2.12.3.Final released</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-12-3-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-12-3-final-released/</id><updated>2022-09-21T00:00:00Z</updated><published>2022-09-21T00:00:00Z</published><summary type="html">Today, we released Quarkus 2.12.3.Final, with a new round of bugfixes and documentation improvements. It is a safe upgrade for anyone using 2.12. Migration Guide If you are not already using 2.12, please refer to our migration guide. Full changelog You can get the full changelog of 2.12.3.Final on GitHub....</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-09-21T00:00:00Z</dc:date></entry><entry><title type="html">Transparent ML, integrating Drools with AIX360</title><link rel="alternate" href="https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/09/transparent-ml-integrating-drools-with-aix360.html</id><updated>2022-09-20T12:10:33Z</updated><content type="html">Following up from about integrating Drools with the Open Prediction Service, in this new post we want to share the current results from another exploration work: this time integrating Drools with research on Transparent Machine Learning by IBM. INTRODUCTION Transparency is a key requirement in many business sectors, from FSI (Financial Services Industry), to Healthcare, to Government institutions, and many others. In more recent years, a generalized need for increased transparency in the decision making processes has gained a great deal of attention from several different stakeholders, especially when it comes to automated decisioning and AI-based decision services. Specifically in the Eurozone, this ties with the and the requirement for explainability in the way businesses automate processes and decision making. Additionally, an “” is proposed and currently under discussion at the European Commission: under the current status of the proposal several risk levels are identified. The integration of AI in the business process and decision model will likely require explainability, transparency and a conformity assessment, depending on the applicable risk level: In other parts of the world, similar legislations are coming into effect or are currently being proposed. You can read more details in . With these considerations in mind, we will explore how to leverage rule induction strategies and specific types of machine learning models, with the intent of producing predictive models which can integrate with effective results into this general context. TRANSPARENT ML WITH DROOLS AND AIX360 One way to address some of the problems and requirements highlighted in the previous section is to use Machine Learning to generate specific types of models that are inherently readable and transparent. As we will see in this blog post, a transparent predictive model can be handed over easily to the next phase as a decision model, in order to be evaluated as-is, but most importantly for the ability to be inspected and authored directly! Comparing a Transparent ML approach with the broader general Machine Learning, we can highlight some of its characteristics: General Machine Learning evaluation:Transparent ML approach:All supported model types, but black box evaluationModel can be inspected, authored, evaluatedAccuracy focusedTransparency focusedeXplainable AI complements, such as Intrinsically eXplainableMLOps —governed by data scienceBusiness centric governanceMultiple runtimesPotentially single runtime Naturally the transparent ML approach has its limitations; we will discuss alternative approaches in the conclusions of this blog post. An example pipeline can be summarized as follows: For the examples in this blog post, we will use the dataset  (predicting if income exceeds $50K/yr from census data). Let’s get started! RULE SET INDUCTION In this section we will make use of the , an open-source library that supports interpretability and explainability of datasets and machine learning models. Our goal in this phase is to generate a predictive model from the UCI Adult dataset, using Machine Learning techniques: To generate a transparent predictive model, we can drive the generation of a RuleSet , as explained in the following Jupyter notebook : As a result of this, we have now generated a set of rules, in the form of a PMML RuleSet, which represents the transparent predictive model for the Adult dataset: If you are interested to delve into more details about using AIX360 and related algorithms, you can check out . DROOLS In this section, we will transform the result from the previous steps into an executable decision model, which can also be directly authored. Please note: in a different context, where the only requirement is the execution of predictive models in general, you can simply make reference to the PMML support for Drools from the , or to integration blueprints such as the integration of Drools with IBM Open Prediction Service from a . In this article instead, as premised, we’re interested in the result of a transparent prediction model, which can be fully inspected, authored and (naturally!) evaluated. Specifically, we will transform the transparent predictive model serialized as a RuleSet, into a DMN model with DMN Decision Tables. To perform this transformation, we will make use of the kie-dmn-ruleset2dmn utility; this is available as a developer API, and as a command line utility too. You can download a published version of the command line utility (executable .jar) from ; otherwise, you can lookup a more recent version directly from . To transform the RuleSet file into a DMN model, you can issue the following command: $ java -jar kie-dmn-ruleset2dmn-cli-8.27.0.Beta.jar adult.pmml --output=adult.dmn This will result in a .dmn file generated, which you can author with the Kogito Tooling and evaluate as usual with the ! We can upload the generated .dmn file onto the sandbox: We can make use of the Kie Sandbox extended services, to evaluate locally the DMN model, as-is or authored as needed! It’s interesting to note the static analysis of the DMN decision table identifies potential gaps in the table, and subsumptions in the rules inducted during the Machine Learning phase; this is expected, and can be authored directly depending on the overall business requirements. From the model evaluation perspective, overlapping rules are not a problem, as they would evaluate to the same prediction; this is a quite common scenario when the ML might have identified overlapping “clusters” or grouping over a number of features, leading to the same output. From a decision table perspective however, overlapping rules can be simplified, as a more compact representation of the same table semantic is often preferable in decision management. Here it is up to the business to decide if to keep the table as translated from the original predictive model, or to leverage the possibilities offered by the transparent ML approach, and simplify/compact the table for easier read and maintenance by the business analyst. DEPLOY We can deploy directly from the KIE Sandbox: Our Transparent prediction and decision model is available as a deployment on OpenShift ! As you can see, with just the click of a button in the KIE Sandbox, our transparent ML model has been easily deployed on OpenShift. If you want to leverage the serverless capabilities of Knative for auto-scaling (including auto scale to zero!) for the same predictive model, you can consider packaging it as a Kogito application. You can find more information in this . CONCLUSION We have seen how a Transparent ML approach can provide solutions to some of the business requirements and conformance needs to regulations such as GDPR or AI Act; we have seen how to drive rule induction by generating predictive models which are inherently transparent, can be authored directly as any other decision model, and can be deployed on a cloud-native OpenShift environment. In this post, we have focused ourselves on using directly upstream AIX360 and Drools. You can refer to the above diagram for commercial solutions by IBM and Red Hat that include these projects too, such as , , . If you are interested in additional capabilities for eXplainable AI solutions, check-out the ! The Transparent ML predictive model, now available as a decision service, can be integrated in other DMN models and other applications, as needed. For example, the transparent prediction on the Adult dataset (predicting if income exceeds $50K/yr) could become invocable as part of another decision service that decides on the applicability for the requests of issuing a certain type of credit card. Another possible integration could be to employ a transparent ML predictive model in the form of scorecards, inside a broader DMN model for segmentation; that is, first identify the applicable category/segment based on the input data, and then apply one of several score cards for the specific segment. Don’t miss on checking out the on related Transparent ML topics! Hope you have enjoyed this blog post, showcasing integration of several technologies to achieve a transparent ML solution! Questions? Feedback? Let us know with the comment section below! Special thanks for Greger Ottosson and Tibor Zimanyi for their help while crafting this content. The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>How hashing and cryptography made the internet possible</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/20/how-hashing-and-cryptography-made-internet-possible" /><author><name>Andy Oram</name></author><id>60312bd5-c40d-4f54-9a4e-fbce728d8518</id><updated>2022-09-20T07:00:00Z</updated><published>2022-09-20T07:00:00Z</published><summary type="html">&lt;p&gt;A lot of technologies, business choices, and public policies gave us the internet we have today—a tremendous boost to the spread of education, culture, and commerce, despite its well-documented flaws. But few people credit two deeply buried technologies for making the internet possible: hashing and cryptography.&lt;/p&gt; &lt;p&gt;If more people understood the role these technologies play, more money and expertise would go toward uncovering and repairing security flaws. For instance, we probably would have fixed the &lt;a href="http://heartbleed.com/"&gt;Heartbleed&lt;/a&gt; programming error much earlier and avoided widespread vulnerabilities in encrypted traffic.&lt;/p&gt; &lt;p&gt;This article briefly explains where hashing and cryptography come from, how they accomplish what they do, and their indelible effect on the modern internet.&lt;/p&gt; &lt;h2&gt;Hashing&lt;/h2&gt; &lt;p&gt;Hashing was &lt;a href="https://www.geeksforgeeks.org/importance-of-hashing/"&gt;invented in the 1950s&lt;/a&gt; at the world's pre-eminent computer firm of that era, IBM, by Hans Peter Luhn. What concerned him at the time was not security—how many computer scientists thought about that?—but saving disk space and memory, the most costly parts of computing back then.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;hash&lt;/em&gt; is a way of reducing each item of data to a small, nearly unique, semi-random string of bits. For instance, if you are storing people's names, you could turn each name into the numerical value of the characters and run a set of adds, multiplies, and shift instructions to produce a 16-bit value. If the hash is good, there will be very few names that produce the same 16-bit value—very few &lt;em&gt;collisions&lt;/em&gt;, as that situation is called.&lt;/p&gt; &lt;p&gt;Now suppose you want to index a database for faster searching. Instead of indexing the names directly, it's much simpler and more efficient to make the index out of 16-bit values. That was one of the original uses for hashes. But they turned out to have two properties that make them valuable for security: No one can produce the original value from the hash, and no one can substitute a different value that produces the same hash. (It is theoretically possible to do either of those things, but doing so would be computationally infeasible, so they're impossible in practice.)&lt;/p&gt; &lt;p&gt;Early Unix systems made use of this property to preserve password security. You created a password along with your user account and gave it to the computer, but the operating system never stored the password itself—it stored only a hash. Every time you entered your password after that, the operating system ran the hash function and let you log in if the resulting hash matched the one in the system. If the password file were snatched up by a malicious intruder, all they would get is a collection of useless hashes. (This clever use of hashes eventually turned out not to be secure enough, so it was replaced with &lt;em&gt;encryption,&lt;/em&gt; which we'll discuss in more detail in the next section of this article.)&lt;/p&gt; &lt;p&gt;Hashes are also good for ensuring that no one has tampered with a document or software program. Injecting malware into free software on popular repositories is not just a theoretical possibility—&lt;a href="https://github.blog/2022-05-26-npm-security-update-oauth-tokens/"&gt;it can actually happen&lt;/a&gt;. Therefore, every time a free software project releases code, the team runs it through a hash function. Every user who downloads the software can run it through the same function to make sure nobody has intercepted the code and inserted malware. If someone changed even one bit and ran the hash function, the resulting hash would be totally different.&lt;/p&gt; &lt;p&gt;Git is another of the myriad tools that use hashes to ensure the integrity of the repository, as well as to enable quick checks on changes to the repository. You can see a hash (a string of random characters) each time you issue a push or log command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;commit 2de089ad3f397e735a45dda3d52d51ca56d8f19a Author: Andy Oram &lt;andyo@example.com&gt; Date: Sat Sep 3 16:28:41 2022 -0400 New material related to commercialization of cryptography. commit f39e7c87873a22e3bb81884c8b0eeeea07fdab48 Author: Andy Oram &lt;andyo@example.com&gt; Date: Fri Sep 2 07:47:42 2022 -0400 Fixed typos. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hash functions can be broken, so &lt;a href="https://valerieaurora.org/hash.html"&gt;new ones are constantly being invented&lt;/a&gt; to replace the functions that are no longer safe.&lt;/p&gt; &lt;h2&gt;Cryptography&lt;/h2&gt; &lt;p&gt;Mathematically speaking, the goal of cryptography has always been to produce output where each bit or character has an equal chance of being another character. If someone intercepted a message and saw the string "xkowpvi," the "x" would have an equal chance of representing an A, a B, a C, and so on.&lt;/p&gt; &lt;p&gt;In digital terms, every bit in an encrypted message has a 50% chance of representing a 0 and a 50% chance of representing a 1.&lt;/p&gt; &lt;p&gt;This goal is related to hashing, and there is a lot of overlap between the fields. Security experts came up with several good ways to create encrypted messages that couldn't be broken—that is, where the decryption process would be computationally infeasible without knowing the secret key used to encrypt the message. But for a long time these methods suffered from an "initial exchange" problem: The person receiving the message needed to somehow also learn what that secret encryption key was, and learn it in a way that didn't reveal the key to anybody else. Whether you're a spy in World War II Berlin trying to communicate with your U.S. buddies, or a modern retail site trying to confirm a customer's credit card online, getting the shared secret securely is a headache.&lt;/p&gt; &lt;p&gt;The solution by now is fairly familiar. The solution creates a pair of keys, one of which you keep private and the other of which you can share freely. Like a hash, the public key is opaque, and no one can determine your private key from it. (The number of bits in the key has to be doubled every decade or so as computers get more powerful.) This solution is generally &lt;a href="https://cryptography.fandom.com/wiki/Diffie%E2%80%93Hellman_key_exchange"&gt;attributed to Whitfield Diffie, Martin Hellman, and Ralph Merkle&lt;/a&gt;, although a British intelligence agent thought of the solution earlier and kept it secret.&lt;/p&gt; &lt;p&gt;Diffie in particular was acutely conscious of social and political reasons for developing public key encryption. In the 1970s, I think that few people thought of doing online retail sales or services using encryption. It was considered a tool of spies and criminals—but also of political dissidents and muckraking journalists. These associations explain why the U.S. government tried to suppress it, or at least keep it from being exported, for decades.&lt;/p&gt; &lt;p&gt;Diffie is still quite active in the field. The most recent article I've seen with him listed as an author was published on July 18, 2022.&lt;/p&gt; &lt;p&gt;The linchpin of internet cryptography came shortly afterward with &lt;a href="https://www.telsy.com/rsa-encryption-cryptography-history-and-uses/"&gt;RSA encryption&lt;/a&gt;, invented by Ron Rivest, Adi Shamir, and Len Adleman. RSA encryption lets two parties communicate without previously exchanging keys, even public keys. (They were prevented from reaping much profit from this historic discovery because the U.S. government prevented the export of RSA technology during most of the life of their patent.)&lt;/p&gt; &lt;p&gt;A big problem in key exchange remains: If someone contacts you and says they are Andy Oram, proffering what they claim to be Andy Oram's public key, how do you know they're really me? The two main solutions (web of trust and certificate authorities) are beyond the scope of this article, and each has vulnerabilities and a lot of overhead. Nevertheless, the internet seems to work well enough with certificate authorities.&lt;/p&gt; &lt;h2&gt;The internet runs on hashes and cryptography&lt;/h2&gt; &lt;p&gt;The internet essentially consists of huge computer farms in data centers, to which administrators and other users have to log in. For many years, the universal way to log into another system was Telnet, now abandoned almost completely because it's insecure. If you use Telnet, someone down the hall can watch your password cross the local network and steal the password. Anyone else who can monitor the network could do the same.&lt;/p&gt; &lt;p&gt;Nowadays, all communication between users and remote computers goes over the secure shell protocol (SSH), which was invented &lt;a href="https://www.oreilly.com/library/view/ssh-the-secure/0596008953/ch01s05.html"&gt;as recently as 1995&lt;/a&gt;. All the cloud computing and other data center administration done nowadays depend on it.&lt;/p&gt; &lt;p&gt;Interestingly, 1995 also saw the advent of the &lt;a href="https://www.techtarget.com/searchsecurity/definition/Secure-Sockets-Layer-SSL"&gt;secure sockets layer&lt;/a&gt; (SSL) protocol, which marks the beginning of web security. Now upgraded to Transport Layer Security (TLS), this protocol is used whenever you enter a URL beginning with HTTPS instead of HTTP. The protocol is so important that &lt;a href="https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html"&gt;Google penalizes web sites that use unencrypted HTTP&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because most APIs now use web protocols, TLS also protects distributed applications. In addition to SSH and TLS, encryption can be found everywhere modern computer systems or devices communicate. That's because the modern internet is beset with attackers, and we use hashes and encryption to minimize their harm.&lt;/p&gt; &lt;p&gt;Some observers think that quantum computing will soon have the power to break encryption as we know it. That could leave us in a scary world: Everything we send over the wire would be available to governments or large companies possessing quantum computers, which are hulking beasts that need to be refrigerated to within a few degrees of absolute zero. We may soon need a &lt;a href="https://nakedsecurity.sophos.com/2022/08/03/post-quantum-cryptography-new-algorithm-gone-in-60-minutes/"&gt;new army of Luhns, Diffies, and other security experts&lt;/a&gt; to find a way to save the internet as we know it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/20/how-hashing-and-cryptography-made-internet-possible" title="How hashing and cryptography made the internet possible"&gt;How hashing and cryptography made the internet possible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andy Oram</dc:creator><dc:date>2022-09-20T07:00:00Z</dc:date></entry><entry><title>Quarkus Tools for IntelliJ 1.13.0 released!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/&#xA;            " /><author><name>Jeff Maury (https://twitter.com/jeffmaury)</name></author><id>https://quarkus.io/blog/intellij-quarkus-tools-1.13.0/</id><updated>2022-09-20T00:00:00Z</updated><published>2022-09-20T00:00:00Z</published><summary type="html">We are very pleased to announce the 1.13.0 release of Quarkus Tools for IntelliJ. This release improves the Qute developer experience. Improved Qute developer experience InlayHint support InlayHint is a new feature. It allows to add inline information about parameters and variables. This is very useful for Qute templates to...</summary><dc:creator>Jeff Maury (https://twitter.com/jeffmaury)</dc:creator><dc:date>2022-09-20T00:00:00Z</dc:date></entry><entry><title>Best ways to learn about Linux from Red Hat Developer</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/19/best-ways-learn-about-linux-red-hat-developer" /><author><name>Heiker Medina</name></author><id>02dd2697-1069-4f18-a29a-6798c4e87270</id><updated>2022-09-19T07:00:00Z</updated><published>2022-09-19T07:00:00Z</published><summary type="html">&lt;p&gt;Looking for tips and deep dives on &lt;a href="https://developers.redhat.com/topics/linux" target="_blank"&gt;Linux&lt;/a&gt;, including &lt;a href="https://developers.redhat.com/products/rhel/overview" target="_blank"&gt;Red Hat Enterprise Linux&lt;/a&gt;? Red Hat Developer has a wide range of content for you. These are some of our favorite and most popular articles, cheat sheets, and lessons to help you get the most out of Linux, along with underlying tools like GCC and Linux-based platforms like Docker:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/learn/lessons/linux-commands"&gt;Helpful Linux commands&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Don't know where to get started in RHEL? This interactive lesson schools you in a series of must-know commands.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/10/modular-perl-red-hat-enterprise-linux-8"&gt;Modular Perl in Red Hat Enterprise Linux 8&lt;/a&gt; (Author: Petr Pisar)&lt;/p&gt; &lt;p&gt;Red Hat Enterprise Linux comes with &lt;em&gt;modules,&lt;/em&gt; a packaging concept allowing system administrators to select the desired software from multiple packaged versions. Learn how to manage Perl as a module, as well as how to manage CPAN modules provided by Perl.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/cheat-sheets/intermediate-linux-cheat-sheet"&gt;Intermediate Linux Cheat Sheet&lt;/a&gt; (Author: Alex Soto Bueno and Bob Reselman)&lt;/p&gt; &lt;p&gt;Ready to level up your Linux knowledge? This cheat sheet presents a collection of Linux commands and executables for developers and system administrators who want to move beyond the basics. You'll find tips on managing processes, users, and groups on Linux, as well as monitoring disk and network usage.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/state-static-analysis-gcc-12-compiler"&gt;The state of static analysis in the GCC 12 compiler&lt;/a&gt; (Author: David Malcolm)&lt;/p&gt; &lt;p&gt;A new feature in the newest version of GCC will help programmers identify and fix two big potential problems: Variables that might have been set to the wrong values and objects that nobody has defined as ready to use.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/06/introduction-linux-bridging-commands-and-features"&gt;An introduction to Linux bridging commands and features&lt;/a&gt; (Author: Hangbin Liu)&lt;/p&gt; &lt;p&gt;How can you use Linux to make a computer work like a switch—that is, a device that connects different devices? A Linux &lt;em&gt;bridge&lt;/em&gt; allows machines to talk to each other even if they're on different internal networks. Learn how to use &lt;code&gt;bridge&lt;/code&gt; and &lt;code&gt;ip link&lt;/code&gt;, the two commands for handling bridges, to build and fix networks.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;Podman basics: Resources for beginners and experts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Podman is a tool for building containers that plays the same role as Docker and is mostly compatible with it. Developers getting started with Podman and those seeking more advanced information should take a deep dive into these resources.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/17/reduce-size-container-images-dockerslim"&gt;Reduce the size of container images with DockerSlim&lt;/a&gt; (Author: Karan Singh)&lt;/p&gt; &lt;p&gt;Using Docker to package your application code together with its dependencies creates a container image. The smaller the container image is, the faster your application will spin up for the first time. But many container images are just too large! This article explains how you can make them smaller so they can load more quickly.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/04/25/podman-basics-cheat-sheet"&gt;Podman basics cheat sheet&lt;/a&gt; (Author: Doug Tidwell)&lt;/p&gt; &lt;p&gt;With help from Podman, Buildah, and Skopeo, you can quickly create and manage rootless containers. This cheat sheet will help you master these tools.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/containers-without-docker"&gt;Containers without docker | DevNation Tech Talk&lt;/a&gt; (Presenter: Cedric Clyburn)&lt;/p&gt; &lt;p&gt;How do you create and run containers without Docker? Red Hat Developer Advocate Cedric Clyburn talks about life after Docker and why containers are here to stay.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/11/easier-way-generate-pdfs-html-templates"&gt;An easier way to generate PDFs from HTML templates&lt;/a&gt; (Author: Muhammad Edwin)&lt;/p&gt; &lt;p&gt;This article shows how to use an open source program, &lt;code&gt;wkhtmltopdf&lt;/code&gt;, to generate PDFs from HTML forms. The example uses Spring Boot, which simplifies Java application development, and Red Hat's &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Images&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/05/extracting-information-python-source-code"&gt;Extracting information from Python source code&lt;/a&gt; (Author: Fridolin Pokorny)&lt;/p&gt; &lt;p&gt;In Python, everything is a symbol—even words like "python" or "import." Invectio is a tool that lets you find all the symbols used or provided by a program. Using state-of-the-art static analysis and machine learning, Invectio can help you better understand software.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/learn/lessons/RHEL-open-lab"&gt;Red Hat Enterprise Linux open lab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explore any Linux topic you choose in this open lab environment designed without pre-planned content.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here are some of our other exciting and top-performing articles–have a look!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/16/code-specialization-mir-lightweight-jit-compiler"&gt;Code specialization for the MIR lightweight JIT compiler&lt;/a&gt; (Author: Vladimir Makarov)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/05/developers-guide-using-kafka-java-part-1"&gt;A developer's guide to using Kafka with Java, Part 1&lt;/a&gt; (Author: Bob Reselman)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/06/07/how-debug-stack-frames-and-recursion-gdb"&gt;How to debug stack frames and recursion in GDB&lt;/a&gt; (Author: Bruno Larsen)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/31/your-go-application-fips-compliant"&gt;Is your Go application FIPS compliant?&lt;/a&gt; (Authors: Antonio Cardace and Sam Fowler)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/23/how-install-command-line-tools-mac"&gt;How to install command-line tools on a Mac&lt;/a&gt; (Author: Varsha Sharma)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Visit our &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux topic page&lt;/a&gt; to discover the most relevant and recent articles on Linux at Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/19/best-ways-learn-about-linux-red-hat-developer" title="Best ways to learn about Linux from Red Hat Developer"&gt;Best ways to learn about Linux from Red Hat Developer&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Heiker Medina</dc:creator><dc:date>2022-09-19T07:00:00Z</dc:date></entry><entry><title>GCC's new fortification level: The gains and costs</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/17/gccs-new-fortification-level" /><author><name>Siddhesh Poyarekar</name></author><id>aa8105eb-3693-4988-8f01-b822ce7471ee</id><updated>2022-09-17T22:00:00Z</updated><published>2022-09-17T22:00:00Z</published><summary type="html">&lt;p&gt;This article describes a new level of fortification supported in GCC. This new level detects more buffer overflows and bugs which mitigates security issues in applications at run time.&lt;/p&gt; &lt;p&gt;C programs routinely suffer from memory management problems. For several years, a &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; preprocessor macro inserted error detection to address these problems at compile time and run time. To add an extra level of security, &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; has been in the GNU C Library (glibc) since version 2.34. I described its mechanisms in my previous blog post, &lt;a href="https://developers.redhat.com/blog/2021/04/16/broadening-compiler-checks-for-buffer-overflows-in-_fortify_source"&gt;Broadening compiler checks for buffer overflows in _FORTIFY_SOURCE&lt;/a&gt;. There has been compiler support for this builtin in &lt;a href="https://clang.llvm.org"&gt;Clang&lt;/a&gt; for some time. Compiler support has also been available for &lt;a href="https://gcc.gnu.org"&gt;GCC&lt;/a&gt; since the release of version 12 in May 2022. The new mitigation should be available in GNU/Linux distributions with packaged GCC 12.&lt;/p&gt; &lt;p&gt;The following sections discuss two principal gains from this enhanced level of security mitigation and the resulting impact on applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2 principal gains:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Enhanced buffer size detection&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Better fortification coverage&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;1. A new builtin provides enhanced buffer size detection&lt;/h2&gt; &lt;p&gt;There is a new builtin underneath the new &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; macro n GCC 12 named &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt;. This builtin is more powerful than the previous &lt;code&gt;__builtin_object_size&lt;/code&gt; builtin used in &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;. When passed a pointer, &lt;code&gt;__builtin_object_size&lt;/code&gt;returns as a compile-time constant that is either the maximum or minimum object size estimate of the object that pointer may be pointing to at that point in the program. On the other hand, &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; is capable of returning a size expression that is evaluated at execution time. Consequently, the &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; builtin detects buffer overflows in many more places than &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The implementation of &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; in GCC is compatible with &lt;code&gt;__builtin_object_size&lt;/code&gt; and thereby interchangeable, especially in the case of fortification. Whenever possible, the builtin computes a precise object size expression. When the builtin does not determine the size exactly, it returns either a maximum or minimum size estimate, depending on the size type argument.&lt;/p&gt; &lt;p&gt;This code snippet demonstrates the key advantage of returning precise values:&lt;/p&gt; &lt;pre&gt;&lt;code class="cpp"&gt;#include &lt;string.h&gt; #include &lt;stdbool.h&gt; #include &lt;stdlib.h&gt; char *b; char buf1[21]; char *__attribute__ ((noinline)) do_set (bool cond) { char *buf = buf1; if (cond) buf = malloc (42); memset (buf, 0, 22); return buf; } int main (int argc, char **argv) { b = do_set (false); return 0; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The program runs to completion when built with &lt;code&gt;-D_FORTIFY_SOURCE=2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gcc -O -D_FORTIFY_SOURCE=2 -o sample sample.c &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But the program aborts when built with &lt;code&gt;-D_FORTIFY_SOURCE=3&lt;/code&gt; and outputs the following message:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;*** buffer overflow detected ***: terminated Aborted (core dumped) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The key enhancement stems from the difference in behavior between &lt;code&gt;__builtin_object_size&lt;/code&gt; and &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt;. &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; uses &lt;code&gt;__builtin_object_size&lt;/code&gt; and returns the maximum estimate for object size at pointer &lt;code&gt;buf&lt;/code&gt;, which is 42. Hence, GCC assumes that the &lt;code&gt;memset&lt;/code&gt; operation is safe at compile time and does not add a call to check the buffer size at run time.&lt;/p&gt; &lt;p&gt;However, GCC with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; invokes &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; to emit an expression that returns the precise size of the buffer that &lt;code&gt;buf&lt;/code&gt; points to at that part in the program. As a result, GCC realizes that the call to &lt;code&gt;memset&lt;/code&gt; might not be safe. Thus, the compiler inserts a call to &lt;code&gt;__memset_chk&lt;/code&gt; into the running code with that size expression as the bound for &lt;code&gt;buf&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;2. Better fortification coverage&lt;/h2&gt; &lt;p&gt;Building distribution packages with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; revealed several issues that &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; missed. Surprisingly, not all of these issues were straightforward buffer overflows. The improved fortification also encountered issues in the GNU C library (glibc) and raised interesting questions about object lifetimes.&lt;/p&gt; &lt;p&gt;Thus, the benefit of improved fortification coverage has implications beyond buffer overflow mitigation. I will explain the outcomes of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; increased coverage in the following sections.&lt;/p&gt; &lt;h3&gt;More trapped buffer overflows&lt;/h3&gt; &lt;p&gt;Building applications with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; detected many simple buffer overflows, such as the &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=2115476"&gt;off-by-one access in clisp&lt;/a&gt; issue. We expected these revelations, which strengthened our justification for building applications with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To further support the use of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; to improve fortification, we used the &lt;a href="https://github.com/siddhesh/fortify-metrics"&gt;Fortify metrics&lt;/a&gt; GCC plugin to estimate the number of times _FORTIFY_SOURCE=3 resulted in a call to a checking function (&lt;code&gt;__memcpy_chk&lt;/code&gt;, &lt;code&gt;__memset_chk&lt;/code&gt;, etc.). We used Fedora test distribution and some of the &lt;code&gt;Server&lt;/code&gt; package group as the sample, which consisted of 96 packages. The key metric is fortification coverage, defined by counting the number of calls to &lt;code&gt;__builtin_object_size&lt;/code&gt; that resulted in a successful size determination and the ratio of this number taken to the total number of &lt;code&gt;__builtin_object_size&lt;/code&gt; calls. The plugin also shows the number of successful calls if using &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt; instead of &lt;code&gt;__builtin_object_size&lt;/code&gt;, allowing us to infer the fortification coverage if all &lt;code&gt;__builtin_object_size&lt;/code&gt; calls were replaced with &lt;code&gt;__builtin_dynamic_object_size&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In this short study, we found that &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; improved fortification by nearly 4 times. For example, the Bash shell went from roughly 3.4% coverage with &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; to nearly 47% with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;. This is an improvement of nearly 14 times. Also, fortification of programs in &lt;code&gt;sudo&lt;/code&gt; went from a measly 1.3% to 49.57% — a jump of almost 38 times!&lt;/p&gt; &lt;h3&gt;The discovery of bugs in glibc&lt;/h3&gt; &lt;p&gt;The increased coverage of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; revealed programming patterns in application programs that tripped over the fortification without necessarily a buffer overflow. While there were some bugs in glibc, we had to either explain why we did not support it or discover ways to discourage those programming patterns.&lt;/p&gt; &lt;p&gt;One example is &lt;code&gt;wcrtomb&lt;/code&gt;, where glibc makes stronger assumptions about the object size passed than POSIX allowed. Specifically, glibc assumes that the buffer passed to &lt;code&gt;wcrtomb&lt;/code&gt; is always at least &lt;code&gt;MB_CUR_MAX&lt;/code&gt; bytes long. In contrast, the POSIX description makes no such assumption. Due to this discrepancy, any application that passed a smaller buffer would potentially make &lt;code&gt;wcrtomb&lt;/code&gt; overflow the buffer during conversion. Then the fortified version &lt;code&gt;__wcrtomb_chk&lt;/code&gt; aborts with a buffer overflow, expecting a buffer that is &lt;code&gt;MB_CUR_MAX&lt;/code&gt; bytes long. We fixed this bug in glibc-2.36 by making glibc conform to POSIX .&lt;/p&gt; &lt;p&gt;&lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; revealed another pattern. Applications such as systemd used &lt;code&gt;malloc_usable_size&lt;/code&gt; to determine available space in objects and then used the residual space. The glibc manual discourages this type of usage, dictating that &lt;code&gt;malloc_usable_size&lt;/code&gt; is for diagnostic purposes only. But applications use the function as a hack to avoid reallocating buffers when there is space in the underlying malloc chunk. The implementation of &lt;code&gt;malloc_usable_size&lt;/code&gt; needs to be fixed to return the allocated object size instead of the chunk size in non-diagnostic use. Alternatively, another solution is to deprecate the function. But that is a topic for discussion by the glibc community.&lt;/p&gt; &lt;h3&gt;Strict C standards compliance&lt;/h3&gt; &lt;p&gt;One interesting use case exposed by &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; raised the question of object lifetimes and what developers can do with freed pointers. The bug in question was in &lt;a href="https://sourceforge.net/p/autogen/bugs/212/"&gt;AutoGen&lt;/a&gt;, using a pointer value after reallocation to determine whether the same chunk extended to get the new block of memory. This practice allowed the developer to skip copying over some pointers to optimize for performance. At the same time, the program continued using the same pointer, not the &lt;code&gt;realloc&lt;/code&gt; call result, since the old pointer did not change.&lt;/p&gt; &lt;p&gt;Seeing that the old pointer continued without an update, the compiler assumed that the object size remained the same. How could it know otherwise? The compiler then failed to account for the reallocation, resulting in an abort due to the perceived buffer overflow.&lt;/p&gt; &lt;p&gt;Strictly speaking, the C standards prohibit using a pointer to an object after its lifetime ends. It should neither be read nor dereferenced. In this context, it is a bug in the application.&lt;/p&gt; &lt;p&gt;However, this idiom is commonly used by developers to prevent making redundant copies. Future updates to &lt;a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=105217"&gt;GCC&lt;/a&gt; may account for this idiom wherever possible, but applications should also explicitly indicate object lifetimes to remain compliant. In the AutoGen example, a simple fix is to unconditionally refresh the pointer after reallocation, ensuring the compiler can detect the new object size.&lt;/p&gt; &lt;h2&gt;The gains of improved security coverage outweigh the cost&lt;/h2&gt; &lt;p&gt;Building with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; may impact the size and performance of the code. Since &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; generated only constant sizes, its overhead was negligible. However, &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; may generate additional code to compute object sizes. These additions may also cause secondary effects, such as register pressure during code generation. Code size tends to increase the size of resultant binaries for the same reason.&lt;/p&gt; &lt;p&gt;We need a proper study of performance and code size to understand the magnitude of the impact created by &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; additional runtime code generation. However the performance and code size overhead may well be worth it due to the magnitude of improvement in security coverage.&lt;/p&gt; &lt;h2&gt;The future of buffer overflow detection&lt;/h2&gt; &lt;p&gt;&lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; has led to significant gains in security mitigation. GCC 12 support brings those gains to distribution builds. But the new level of fortification also revealed interesting issues that require additional work to support correctly. For more background information, check out my previous article, &lt;a href="https://www.redhat.com/en/blog/enhance-application-security-fortifysource"&gt;Enhance application security with FORTIFY_SOURCE&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Object size determination and fortification remain relevant areas for improvements in compiler toolchains. The toolchain team at Red Hat continues to be involved in the GNU and LLVM communities to make these improvements.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/17/gccs-new-fortification-level" title="GCC's new fortification level: The gains and costs"&gt;GCC's new fortification level: The gains and costs&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Siddhesh Poyarekar</dc:creator><dc:date>2022-09-17T22:00:00Z</dc:date></entry><entry><title>My advice for updating use of the Docker Hub OpenJDK image</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/16/updating-docker-hubs-openjdk-image" /><author><name>Tim Ellison</name></author><id>3bcb8704-1585-4386-8123-ee3bcc089043</id><updated>2022-09-16T18:00:00Z</updated><published>2022-09-16T18:00:00Z</published><summary type="html">&lt;p&gt;The Java runtime environment in your containers could stop receiving updates in the coming months. It's time to take action. This article explains the decisions that led to this issue and proposes a solution.&lt;/p&gt; &lt;h2&gt;OpenJDK and Java SE updates&lt;/h2&gt; &lt;p&gt;&lt;a href="https://openjdk.org/"&gt;OpenJDK&lt;/a&gt; is an open source implementation of the Java Platform, Standard Edition (Java SE), on which multiple companies and contributors collaborate.&lt;/p&gt; &lt;p&gt;A project at OpenJDK represents each new feature release of the Java SE specification. Subsequent updates to those features, including functional and security fixes, are led by maintainers working in the &lt;a href="https://openjdk.org/projects/jdk-updates/"&gt;JDK updates project&lt;/a&gt;. Long-term supported releases such as Java SE 8 (since March 2014), Java SE 11 (since Sept 2018), and Java SE 17 (since Sept 2021) undergo a quarterly release update under the guidance of a lead maintainer.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://openjdk.org/projects/jdk-updates/maintainers.html"&gt;repository maintainers' role&lt;/a&gt; is to ensure that updates are both necessary and appropriate for deployed releases. They consider the opinions of multiple contributors when making such update decisions. Many vendors and distributors of Java SE subsequently build from the OpenJDK source code to provide new releases of their own branded Java SE offerings.&lt;/p&gt; &lt;p&gt;Andrew Haley (Red Hat) is the lead maintainer for Java 8 updates and Java 11 updates, and Goetz Lindenmaier (SAP) is the lead maintainer for Java 17 updates. Update maintainers affiliated with companies that provide commercially supported distributions of OpenJDK based on Java SE work as independent contributors to the project.&lt;/p&gt; &lt;h2&gt;Docker Hub deprecates OpenJDK images&lt;/h2&gt; &lt;p&gt;For many years, the official &lt;a href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt; image builders took OpenJDK Java SE update binaries from &lt;a href="https://adoptium.net/"&gt;Eclipse Adoptium&lt;/a&gt; and other locations to build their own image. But in July 2022, the Docker Hub image builders &lt;a href="https://hub.docker.com/_/openjdk"&gt;announced the deprecation&lt;/a&gt; of this popular image.&lt;/p&gt; &lt;p&gt;Now, Docker asks users to obtain their builds of OpenJDK, either from a commercial Java vendor or directly from the Adoptium project. There will be no further updates to the existing OpenJDK image, so users risk falling behind with functional and security updates to their Java SE usage unless they move to an alternate provider. I believe the official &lt;a href="https://hub.docker.com/_/eclipse-temurin"&gt;Eclipse Temurin image&lt;/a&gt; maintained by the Adoptium project is the obvious choice for a replacement image.&lt;/p&gt; &lt;h2&gt;Eclipse Adoptium builds JDKs&lt;/h2&gt; &lt;p&gt;OpenJDK does not provide binary updates directly from the update projects. Since July 2022, these long-term supported Java update projects have depended upon &lt;a href="https://adoptium.net/"&gt;Eclipse Adoptium&lt;/a&gt; to build and distribute consumable OpenJDK binaries.&lt;/p&gt; &lt;p&gt;Adoptium is a project dedicated to building, testing, and distributing up-to-date and ready-to-use OpenJDK binaries under an open source license. Adoptium calls their builds of OpenJDK, Temurin. They are available across a broad range of processors and operating systems. These Temurin binaries have over half a billion downloads and earned the trust of enterprise production environments worldwide. A vendor-independent &lt;a href="https://adoptium.net/members"&gt;working group&lt;/a&gt; based at the Eclipse software foundation leads Adoptium.&lt;/p&gt; &lt;p&gt;The Adoptium community provides binaries built directly from OpenJDK source code. These Temurin binaries are available as direct downloads, installers, or container images and are faithful representations of the OpenJDK update source built under controlled conditions.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://hub.docker.com/_/eclipse-temurin"&gt;official Docker Hub Temurin images&lt;/a&gt; contain the latest releases of the OpenJDK updates for several Java SE versions, thoroughly tested with various applications. The images work as direct drop-in replacements for the OpenJDK images. Some OpenJDK images already contain Temurin binaries.&lt;/p&gt; &lt;h2&gt;How to move from OpenJDK images to Eclipse Temurin images&lt;/h2&gt; &lt;p&gt;The Docker Hub's deprecation decision presents a problem. But there is a solution. We recommend moving from the &lt;a href="https://hub.docker.com/_/openjdk"&gt;OpenJDK image&lt;/a&gt; to &lt;a href="https://hub.docker.com/_/eclipse-temurin"&gt;the official Docker Hub Eclipse Temurin image&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The process is simple. All you have to do is identify the &lt;code&gt;FROM&lt;/code&gt; lines in Dockerfiles such as this:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM: openjdk:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Change the lines as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM eclipse-temurin:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The process for changing the use of images other than version 17 is equivalent. You can &lt;a href="https://github.com/adoptium/adoptium-support/issues"&gt;report&lt;/a&gt; issues to the Adoptium community.&lt;/p&gt; &lt;h2&gt;Red Hat support&lt;/h2&gt; &lt;p&gt;We encourage everyone to switch to Eclipse Temurin. Many &lt;a href="https://github.com/jenkinsci/docker/pull/1429"&gt;application images&lt;/a&gt; and &lt;a href="https://github.com/javastacks/spring-boot-best-practice/blob/fc6709cf2ec2fc00b4dfae7210ce503f9c10560c/spring-boot-docker/Dockerfile"&gt;examples of best practices&lt;/a&gt; have successfully made the change.&lt;/p&gt; &lt;p&gt;Red Hat recently &lt;a href="https://developers.redhat.com/articles/2022/08/24/red-hat-expands-support-java-eclipse-temurin"&gt;announced direct support for Temurin&lt;/a&gt; in development and production as part of Red Hat Runtimes, Red Hat OpenShift, and Red Hat Build of OpenJDK. Red Hat support assures customers that the move to Temurin will be smooth, allowing you to continue focusing on building products that integrate and automate modern business applications and processes.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/16/updating-docker-hubs-openjdk-image" title="My advice for updating use of the Docker Hub OpenJDK image"&gt;My advice for updating use of the Docker Hub OpenJDK image&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tim Ellison</dc:creator><dc:date>2022-09-16T18:00:00Z</dc:date></entry></feed>
